# Classic Models Pipeline

  ```
 STILL IN PROGRESS
  ```

This project involves periodically extracting data from a normalised, operational database (classic model cars) into a Data Warehouse.

This process leverages cloud-based tools such as Azure Data Factory and Databricks to efficiently handle data extraction, transformation, and loading (ETL). The end result is a star schema designed to enhance data accessibility and performance for analytical queries.

In Addition, the project incorporates Machine Learning and Data Analytics components, leveraging tools such as Flask for model deployment and Power BI for data visualisation and reporting.

## Architecutre / Process

```
STILL IN PROGRESS
```

1. Design [Star Schema Dimensional Model](https://www.databricks.com/glossary/star-schema#:~:text=A%20star%20schema%20is%20a,for%20querying%20large%20data%20sets.)
1. Extract & Load data into [Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)
1. Transform with [Databricks](https://www.databricks.com)
1. Load into [Delta Lake](https://delta.io) Star Schema.
1. Orchestrate with [Azure Data Factory (ADF)](https://azure.microsoft.com/en-gb/products/data-factory)
1. Visualising data with [Power BI](https://www.microsoft.com/en-us/power-platform/products/power-bi)
1. Develop and deploy [Machine Learning Model](https://www.ibm.com/topics/machine-learning) with [Flask](https://flask.palletsprojects.com/en/3.0.x/).

## Output

```
STILL IN PROGRESS
```

## Setup

```
STILL IN PROGRESS
```

Follow the below steps to setup pipeline. 

1. Overview
1. Setup Azure Services